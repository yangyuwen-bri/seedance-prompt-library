#!/usr/bin/env python3
"""
ä» all_tweets.json ä¸­æå–åŒ…å«å®Œæ•´ prompt + è§†é¢‘çš„æ¨æ–‡ï¼Œ
ç”Ÿæˆ prompt_library.jsonã€‚

ç­›é€‰é€»è¾‘ï¼š
1. æ¨æ–‡åŒ…å«æ˜ç¡®çš„ prompt æ–‡æœ¬
2. æ¨æ–‡é™„å¸¦è§†é¢‘å†…å®¹
3. æ’é™¤ Grok è‡ªåŠ¨å›å¤ã€æ–°é—»è½¬è¿°
4. å»é‡ï¼šç›¸åŒ prompt ä¿ç•™äº’åŠ¨é‡æœ€é«˜çš„
"""

import os
import json
import re
from collections import defaultdict

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


def has_video(tweet):
    """æ£€æŸ¥æ¨æ–‡æ˜¯å¦é™„å¸¦è§†é¢‘"""
    media = tweet.get('media', [])
    if isinstance(media, list):
        for m in media:
            if isinstance(m, str):
                if 'video_thumb' in m or 'amplify_video' in m or 'ext_tw_video' in m:
                    return True
            elif isinstance(m, dict):
                if m.get('type') == 'video' or 'video' in str(m.get('url', '')):
                    return True
    # æ£€æŸ¥æ­£æ–‡ä¸­çš„é“¾æ¥
    text = tweet.get('text', '')
    if re.search(r'https://t\.co/\w+', text):
        return True
    return False


def get_video_thumbnail(tweet):
    """è·å–è§†é¢‘ç¼©ç•¥å›¾ URL"""
    media = tweet.get('media', [])
    if isinstance(media, list):
        for m in media:
            if isinstance(m, str) and ('video_thumb' in m or 'amplify_video' in m or 'ext_tw_video' in m):
                return m
            elif isinstance(m, dict):
                return m.get('thumbnail', m.get('url', ''))
    return ''


def extract_prompt_text(text):
    """ä»æ¨æ–‡å…¨æ–‡ä¸­æå– prompt å†…å®¹"""
    if not text:
        return None

    text_clean = text.replace('""', '"')

    # Pattern 1: Prompt: "xxx" æˆ– Prompt: xxx
    prompt_patterns = [
        r'[Pp][Rr][Oo][Mm][Pp][Tt]\s*(?:\(.*?\))?\s*[:ï¼š]\s*["""\'](.+?)["""\']',
        r'[Pp][Rr][Oo][Mm][Pp][Tt]\s*(?:\(.*?\))?\s*[:ï¼š]\s*(.+?)(?:\n\n|https://|$)',
        r'PROMPT\s*[:ï¼š]\s*["""\'](.+?)["""\']',
    ]

    for pattern in prompt_patterns:
        match = re.search(pattern, text_clean, re.DOTALL | re.IGNORECASE)
        if match:
            prompt = match.group(1).strip()
            prompt = re.sub(r'\s*#\w+.*$', '', prompt, flags=re.DOTALL)
            prompt = re.sub(r'\s*https://t\.co/\S+', '', prompt)
            prompt = prompt.strip().strip('"').strip("'").strip('\u201c').strip('\u201d')
            if len(prompt) > 10:
                return prompt

    # Pattern 2: å¼•å·åŒ…è£¹çš„å†…å®¹
    text_lower = text_clean.lower()
    if 'seedance' in text_lower:
        quoted = re.findall(r'"([^"]{15,})"', text_clean)
        if not quoted:
            quoted = re.findall(r'\u201c([^\u201d]{15,})\u201d', text_clean)
        if quoted:
            longest = max(quoted, key=len)
            if len(longest) > 15:
                return longest.strip()

    # Pattern 3: JSON æ ¼å¼
    if '{' in text_clean and 'title' in text_lower:
        json_match = re.search(r'\{[\s\S]+\}', text_clean)
        if json_match and len(json_match.group(0)) > 50:
            return json_match.group(0).strip()

    # Pattern 4: ä¸­æ–‡ç»“æ„åŒ– prompt
    if 'ã€' in text_clean and ('prompt' in text_lower or 'æ–‡ç”Ÿè§†é¢‘' in text_clean):
        struct_match = re.search(r'(ã€.+)', text_clean, re.DOTALL)
        if struct_match:
            prompt = struct_match.group(1).strip()
            prompt = re.sub(r'\s*#\w+.*$', '', prompt, flags=re.DOTALL)
            prompt = re.sub(r'\s*https://t\.co/\S+', '', prompt)
            if len(prompt) > 20:
                return prompt.strip()

    return None


def is_grok_response(tweet):
    """æ£€æŸ¥æ˜¯å¦ä¸º Grok è‡ªåŠ¨å›å¤"""
    author = tweet.get('author', {})
    if isinstance(author, dict):
        return author.get('userName', '').lower() == 'grok'
    return False


def is_news_repost(text):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ–°é—»è½¬è¿°"""
    if not text:
        return False
    indicators = [
        'Chinese company ByteDance released',
        'Someone tested the new version',
        'It is impossible to distinguish',
        'just 48 hours ago',
        'Lu Huang, an AI consultant',
    ]
    return sum(1 for ind in indicators if ind.lower() in text.lower()) >= 2


def get_engagement(tweet):
    """è®¡ç®—äº’åŠ¨åˆ†æ•°"""
    likes = int(tweet.get('likeCount', 0) or 0)
    rts = int(tweet.get('retweetCount', 0) or 0)
    replies = int(tweet.get('replyCount', 0) or 0)
    quotes = int(tweet.get('quoteCount', 0) or 0)
    bookmarks = int(tweet.get('bookmarkCount', 0) or 0)
    return likes + rts * 2 + replies * 0.5 + quotes * 1.5 + bookmarks


def normalize_prompt(prompt):
    """å½’ä¸€åŒ– prompt ç”¨äºå»é‡"""
    if not prompt:
        return ''
    norm = prompt.lower().strip()
    norm = re.sub(r'[^\w\s]', '', norm)
    norm = re.sub(r'\s+', ' ', norm)
    return norm[:100]


def extract_prompts(input_file=None):
    """ä¸»æµç¨‹ï¼šä»å…¨é‡æ¨æ–‡ä¸­æå– prompt ç´ æ"""
    if input_file is None:
        input_file = os.path.join(BASE_DIR, 'data', 'all_tweets.json')

    output_file = os.path.join(BASE_DIR, 'data', 'prompt_library.json')

    with open(input_file, 'r', encoding='utf-8') as f:
        tweets = json.load(f)

    print(f"ğŸ“Š æ€»æ¨æ–‡æ•°: {len(tweets)}")

    results = []
    stats = {'grok': 0, 'news': 0, 'no_video': 0, 'no_prompt': 0, 'blacklisted': 0}

    # åŠ è½½é»‘åå•
    blacklist_file = os.path.join(BASE_DIR, 'data', 'blacklist.txt')
    blacklist = set()
    if os.path.exists(blacklist_file):
        with open(blacklist_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'):
                    blacklist.add(url)
        print(f"ğŸš« åŠ è½½é»‘åå•: {len(blacklist)} æ¡")

    for tweet in tweets:
        text = tweet.get('text', '')
        url = tweet.get('url', '') # Use 'url' field for consistency with blacklist

        # è¿‡æ»¤é»‘åå•
        if url and url in blacklist:
            stats['blacklisted'] += 1
            continue

        if is_grok_response(tweet):
            stats['grok'] += 1
            continue

        if is_news_repost(text):
            stats['news'] += 1
            continue

        if not has_video(tweet):
            stats['no_video'] += 1
            continue

        prompt = extract_prompt_text(text)
        if not prompt:
            stats['no_prompt'] += 1
            continue

        author = tweet.get('author', {})
        engagement = get_engagement(tweet)

        results.append({
            'prompt': prompt,
            'prompt_length': len(prompt),
            'tweet_url': tweet.get('url', ''),
            'author': author.get('userName', ''),
            'author_name': author.get('name', ''),
            'author_followers': author.get('followers', 0),
            'created_at': tweet.get('createdAt', ''),
            'lang': tweet.get('lang', ''),
            'likes': int(tweet.get('likeCount', 0) or 0),
            'retweets': int(tweet.get('retweetCount', 0) or 0),
            'replies': int(tweet.get('replyCount', 0) or 0),
            'bookmarks': int(tweet.get('bookmarkCount', 0) or 0),
            'engagement_score': engagement,
            'video_thumbnail': get_video_thumbnail(tweet),
            'full_text_preview': (text[:200] + '...') if len(text) > 200 else text,
            # ä»¥ä¸‹å­—æ®µç”± classify_prompts.py å¡«å……
            'tags': [],
            'quality_score': 0,
            'summary': '',
        })

    print(f"  æ’é™¤ Grok å›å¤: {stats['grok']}")
    print(f"  æ’é™¤æ–°é—»è½¬è¿°: {stats['news']}")
    print(f"  æ’é™¤æ— è§†é¢‘: {stats['no_video']}")
    print(f"  æ’é™¤æ—  prompt: {stats['no_prompt']}")
    print(f"  åˆæ­¥åŒ¹é…: {len(results)}")

    # å»é‡
    groups = defaultdict(list)
    for r in results:
        groups[normalize_prompt(r['prompt'])].append(r)

    deduplicated = []
    dup_count = 0
    for group in groups.values():
        group.sort(key=lambda x: x['engagement_score'], reverse=True)
        deduplicated.append(group[0])
        dup_count += len(group) - 1

    deduplicated.sort(key=lambda x: x['engagement_score'], reverse=True)

    print(f"  å»é‡ç§»é™¤: {dup_count}")
    print(f"  âœ… æœ€ç»ˆç´ ææ•°: {len(deduplicated)}")

    # ä¿å­˜
    library = {
        'metadata': {
            'total_tweets': len(tweets),
            'prompts_extracted': len(deduplicated),
            'last_updated': '',
            'description': 'Seedance Prompt Library - AI video prompt examples with results',
        },
        'prompts': deduplicated,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(library, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å·²ä¿å­˜: {output_file}")
    return output_file


if __name__ == '__main__':
    extract_prompts()
