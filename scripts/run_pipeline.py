#!/usr/bin/env python3
"""
ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹ï¼š
fetch â†’ merge â†’ extract â†’ classify â†’ generate
"""

import argparse
import sys
import os

# å°† scripts ç›®å½•åŠ å…¥ path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from fetch_tweets import fetch_tweets
from merge_dedup import merge_and_dedup
from extract_prompts import extract_prompts
from classify_prompts import classify_prompts
from generate_site import generate_site


def run_pipeline(skip_fetch=False, skip_classify=False, days_back=1, max_items=4000):
    """è¿è¡Œå®Œæ•´ pipeline"""
    print("=" * 60)
    print("ğŸš€ Seedance Prompt Library Pipeline")
    print("=" * 60)

    # Step 1: é‡‡é›†
    if not skip_fetch:
        print("\n" + "=" * 60)
        print("ğŸ“¡ Step 1/5: é‡‡é›†æ¨æ–‡")
        print("=" * 60)
        fetch_tweets(days_back=days_back, max_items=max_items)
    else:
        print("\nâ­ï¸  è·³è¿‡é‡‡é›†æ­¥éª¤")

    # Step 2: åˆå¹¶å»é‡
    print("\n" + "=" * 60)
    print("ğŸ”„ Step 2/5: åˆå¹¶å»é‡")
    print("=" * 60)
    merge_and_dedup()

    # Step 3: æå– prompt
    print("\n" + "=" * 60)
    print("ğŸ” Step 3/5: æå– Prompt")
    print("=" * 60)
    extract_prompts()

    # Step 4: åˆ†ç±»
    if not skip_classify:
        print("\n" + "=" * 60)
        print("ğŸ·ï¸  Step 4/5: Gemini æ™ºèƒ½åˆ†ç±»")
        print("=" * 60)
        classify_prompts()
    else:
        print("\nâ­ï¸  è·³è¿‡åˆ†ç±»æ­¥éª¤")

    # Step 5: ç”Ÿæˆç«™ç‚¹
    print("\n" + "=" * 60)
    print("ğŸŒ Step 5/5: ç”Ÿæˆå±•ç¤ºé¡µé¢")
    print("=" * 60)
    generate_site()

    print("\n" + "=" * 60)
    print("âœ… Pipeline å®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run the full Seedance prompt library pipeline')
    parser.add_argument('--skip-fetch', action='store_true', help='Skip tweet fetching (use existing data)')
    parser.add_argument('--skip-classify', action='store_true', help='Skip Gemini classification')
    parser.add_argument('--days', type=int, default=1, help='Days back to fetch')
    parser.add_argument('--max', type=int, default=4000, help='Max tweets to fetch')
    args = parser.parse_args()

    run_pipeline(
        skip_fetch=args.skip_fetch,
        skip_classify=args.skip_classify,
        days_back=args.days,
        max_items=args.max,
    )
